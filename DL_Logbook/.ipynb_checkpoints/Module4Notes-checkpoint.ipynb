{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f888876-fc4b-4944-82bd-180380ec0d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Module 4 Notes: Training Deep Neural Networks & Regularization\n",
    "\n",
    "# TensorFlow is a framework for building and optimizing deep neural networks. It handles...\n",
    "\n",
    "# Automatic differentiation\n",
    "# Tensorflow computation (like NumPy but GPU accelerated)\n",
    "# Model training and graph execution\n",
    "\n",
    "\n",
    "\n",
    "# %pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "c = a + b\n",
    "print(c)\n",
    "\n",
    "# tf.constant() creates immutable tensors\n",
    "\n",
    "# TensorFlow automatically builds a computational graph\n",
    "\n",
    "# Operations like a + b are tracked for gradient computation\n",
    "\n",
    "# TensorFlow automatically manages data types (float32, int32) and uses GPU if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a878d731-47e0-4681-aff9-d700425bc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Variable path=dense_1/kernel, shape=(2, 3), dtype=float32, value=[[ 0.8272064  -0.03398919 -0.48470825]\n",
      " [-0.53453773 -0.54874766 -0.58118814]]>, <Variable path=dense_1/bias, shape=(3,), dtype=float32, value=[0. 0. 0.]>]\n"
     ]
    }
   ],
   "source": [
    "# Trainable Weights & Model Parameters\n",
    "\n",
    "# In Keras, every layer has weights (trainable) and sometimes biases\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "layer = layers.Dense(3, input_shape=(2,))\n",
    "layer.build(input_shape=(None, 2))\n",
    "print(layer.trainable_weights)\n",
    "\n",
    "# A dense layer connects every input neuron to every output neuron. \n",
    "\n",
    "# The layer has:\n",
    "\n",
    "# Kernel (W): weights matrix of shape (input_dim, output_dim)\n",
    "# Bias (b): vector added to each neuron output\n",
    "# trainable_weights returns parameters that are updated during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aace7b36-eae8-49be-953a-220be8a3c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST and the Flatten Layer\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 225.0\n",
    "X_test = X_test / 225.0\n",
    "\n",
    "# MNIST = 70,000 grayscale images of handwritten digits (0-9).\n",
    "# Divided into 60k train, 10k test.\n",
    "# Scaling (0-1) range improves gradient performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986383bd-e0aa-442f-a3af-ad0c1a07b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Flatten converts each 28 x 28 image to 784 element vector\n",
    "# Dense(128) hidden layer with 128 neurons and ReLU activtion\n",
    "# Dense(10) output layer for 10 digits (softmax converts to probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab15b5a1-a9e7-4c1a-a092-a3ceb4884522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 927us/step - accuracy: 0.9265 - loss: 0.2558 - val_accuracy: 0.9587 - val_loss: 0.1365\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 834us/step - accuracy: 0.9665 - loss: 0.1122 - val_accuracy: 0.9688 - val_loss: 0.1035\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 905us/step - accuracy: 0.9764 - loss: 0.0775 - val_accuracy: 0.9750 - val_loss: 0.0817\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 899us/step - accuracy: 0.9824 - loss: 0.0579 - val_accuracy: 0.9766 - val_loss: 0.0762\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830us/step - accuracy: 0.9863 - loss: 0.0441 - val_accuracy: 0.9725 - val_loss: 0.0850\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - accuracy: 0.9885 - loss: 0.0353 - val_accuracy: 0.9749 - val_loss: 0.0805\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 889us/step - accuracy: 0.9908 - loss: 0.0291 - val_accuracy: 0.9761 - val_loss: 0.0811\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 813us/step - accuracy: 0.9926 - loss: 0.0234 - val_accuracy: 0.9774 - val_loss: 0.0733\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 929us/step - accuracy: 0.9937 - loss: 0.0194 - val_accuracy: 0.9768 - val_loss: 0.0789\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715us/step - accuracy: 0.9947 - loss: 0.0165 - val_accuracy: 0.9720 - val_loss: 0.1033\n"
     ]
    }
   ],
   "source": [
    "# Compile and train\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=10)\n",
    "\n",
    "# Loss: sparse categorical cross-entropy, used when labels are integers\n",
    "\n",
    "# Optimizer: Adam adjusts learning rates adaptively\n",
    "\n",
    "# Epochs: Each full pass through training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065fe37c-cca0-47e6-971a-ea0025205e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.9720 - loss: 0.1033\n",
      "0.972000002861023\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(test_acc)\n",
    "\n",
    "# Evaluates on unseen data\n",
    "\n",
    "# Typically > 97% accuracy for this architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4cda9f-64c1-418d-aeb0-0261853eda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization (from regularization.ipynb)\n",
    "\n",
    "# Regularization reduces overfitting by penalizing large weights\n",
    "\n",
    " # L2 Regularization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                 input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Adds ùúÜ‚àëùë§2 to loss\n",
    "\n",
    "# Encourages smaller weights, smoother decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ede1b2b8-eab5-42a7-b850-15dc509e7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64,activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout (0.5),\n",
    "    layers.Dense(10, activation ='softmax')\n",
    "])\n",
    "\n",
    "# Drops 50% of neurons each iteration\n",
    "# Prevents co-dependence between neurons\n",
    "# Applied only during training, not inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e30f90e7-bb0c-450e-ba4b-4745044f537e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Example Initialization\n",
    "\n",
    "initializer = keras.initializers.HeNormal()\n",
    "layer = layers.Dense(64, activation='relu', kernel_initializer=initializer)\n",
    "\n",
    "# Keeps gradient magnitude stable across layers to prevent vanishing/ecploding behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0f172f9-6b70-4146-91fe-e5180bdab7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced training techniques\n",
    "\n",
    "# From Geron_11_training_deep_neural_networks\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# Decreases learning rate over time\n",
    "# Prevents overshooting as model converges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aae87b23-a80b-4c80-996c-1ff5fddef0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Stops training when validation loss doesn't improve.\n",
    "\n",
    "# Restores the weights with the best validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa11426-f222-462d-ae7c-cf1743de7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "model.save('model.h5')\n",
    "reloaded = keras.models.load_model('model.h5')\n",
    "\n",
    "# saves architecrue + weights +optmizer state\n",
    "\n",
    "# HDF5 (.h5) format standard for model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c6e21-64e2-42f3-a92b-051c84d788b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
