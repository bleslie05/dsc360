{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp9ttxMC87tc"
      },
      "source": [
        "## `tensorflow_datasets`,  `tf.data.Datasets` API.\n",
        "\n",
        "This notebook illustrates various \n",
        "\n",
        "`tensorflow_datasets` is a library for accessing and loading the tensorflow library of data sets that we will use from time to time. `tf.data.Datasets`  is an API for creating pipelines within tensorflow modeling ecosystem. Sometimes it will be convenient to use this, and many of the examples in Chollet use it. It is also discussed in Geron. We illustrate how to download data from the tfds library. Those data sets are automatically loaded as `tf.data.Datasets` objects, so we expose a little of that functionality. \n",
        "\n",
        "There are many ways to acquire data and process as `tf.data.Datasets` objects, and you dont need to use `tensorflow_datasets` to do it. Here we illustrate, and show the similarities and differences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XhwkHctY87td"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQRP1xySx_Bf"
      },
      "source": [
        "### `cifar10` data.\n",
        "\n",
        "This is one of the data sets curated in tensorflow. We will use as an example and you also will use in your assignment. This data set consists of image pixel data in 32 x 32 x 3 shape.  This is image data, with ten categories. See the documentation for further information about the data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "train = tfds.load('cifar10', split='train[:20%]', shuffle_files=True, as_supervised= True, download=False)\n",
        "val = tfds.load('cifar10', split='train[20%:30%]', shuffle_files=True, as_supervised=True, download=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "This gives you `tf.data.Dataset` objects. When you use `tfds.load()` with `as_supervised=True`, it returns a `tf.data.Dataset` where each element is a tuple of `(image, label)`, which is ready to use directly with Keras/TensorFlow models.\n",
        "\n",
        "The key points:\n",
        "- `as_supervised=True` ensures you get tuples of `(features, labels)` instead of dictionaries\n",
        "- No need for lambda functions to extract the image and label components\n",
        "- The returned `train` and `val` variables are `tf.data.Dataset` objects\n",
        "- You can then apply additional transformations like `.batch()`, `.shuffle()`, `.prefetch()` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " `take(1)` returns the first batch. The batch size is just 1, so `take(1)` will just be a single example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label value: 7\n",
            "Label shape: ()\n",
            "Label dtype: <dtype: 'int64'>\n",
            "image shape: (32, 32, 3)\n",
            "image dtype: <dtype: 'uint8'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-02 16:56:59.673966: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
            "2025-11-02 16:56:59.674588: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the actual shape and value of a label\n",
        "for image, label in train.take(1):\n",
        "    print(f\"Label value: {label}\")\n",
        "    print(f\"Label shape: {label.shape}\")\n",
        "    print(f\"Label dtype: {label.dtype}\")\n",
        "    print(f\"image shape: {image.shape}\")\n",
        "    print(f\"image dtype: {image.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting single example shape from batched data for neural network input layer\n",
        "\n",
        "# Method 1: From element_spec (before batching)\n",
        "single_image_shape = train.element_spec[0].shape\n",
        "print(f\"Single image shape from element_spec: {single_image_shape}\")\n",
        "\n",
        "# Method 2: Create a small batch and examine\n",
        "batched_train = train.batch(32)\n",
        "print(f\"Batched data element_spec: {batched_train.element_spec}\")\n",
        "\n",
        "# Method 3: Take one batch and get shape of single example\n",
        "for batch_images, batch_labels in batched_train.take(1):\n",
        "    print(f\"Batch shape: {batch_images.shape}\")\n",
        "    print(f\"Single example shape (remove batch dim): {batch_images.shape[1:]}\")\n",
        "    \n",
        "    # This is what you use for your neural network input layer:\n",
        "    input_shape = batch_images.shape[1:]\n",
        "    print(f\"Input shape for NN: {input_shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practical example: Using the shape in a neural network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Get the input shape for a single example (without batch dimension)\n",
        "input_shape = train.element_spec[0].shape  # This gives (32, 32, 3)\n",
        "\n",
        "print(f\"Input shape for model: {input_shape}\")\n",
        "\n",
        "# Example neural network using this shape\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=input_shape),  # (32, 32, 3) - no batch dimension!\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"Model input shape:\", model.input_shape)  # This will show (None, 32, 32, 3)\n",
        "# The None is for the batch dimension - Keras adds it automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train, validation, test \n",
        "\n",
        "To do this we might split the training into validation and train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train = tfds.load('cifar10', split='train[:20%]', shuffle_files=True, as_supervised= True, download=False)\n",
        "val = tfds.load('cifar10', split='train[20%:30%]', shuffle_files=True, as_supervised=True, download=False)\n",
        "test = tfds.load('cifar10', split='train[30:40%]', shuffle_files=True, as_supervised= True, download=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative\n",
        "\n",
        "Finally, if you dont want to use `tensorflow_datasets`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(50000)\n",
        "    .take(5000)          # take only first 5000 examples\n",
        "    .batch(64)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remarks\n",
        "\n",
        "**`tf.data.Dataset.from_tensor_slices()` is for converting in-memory arrays to `tf.data.Dataset` objects.**\n",
        "\n",
        "## Common scenarios:\n",
        "\n",
        "### 1. **Converting NumPy arrays**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You have NumPy arrays\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y = np.array([0, 1, 0])\n",
        "\n",
        "# Convert to tf.data.Dataset for batching/transformations\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 2. **Your highlighted example - Keras datasets**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()  # NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))  # Convert to Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 3. **Converting pandas DataFrames**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "features = df[['feature1', 'feature2']].values\n",
        "labels = df['target'].values\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "## **Why use `from_tensor_slices()`?**\n",
        "\n",
        "✅ **Benefits of converting to `tf.data.Dataset`:**\n",
        "- **Batching**: `.batch(32)`\n",
        "- **Shuffling**: `.shuffle(1000)` \n",
        "- **Transformations**: `.map(preprocess_function)`\n",
        "- **Prefetching**: `.prefetch(tf.data.AUTOTUNE)`\n",
        "- **Performance optimizations**: Parallel processing, memory efficiency\n",
        "- **Consistent API**: Same interface as other TensorFlow datasets\n",
        "\n",
        "## **Summary of the workflow:**\n",
        "1. **Start with**: In-memory arrays (NumPy, lists, tensors)\n",
        "2. **Convert using**: `tf.data.Dataset.from_tensor_slices()`\n",
        "3. **Apply transformations**: `.batch()`, `.shuffle()`, `.map()`, etc.\n",
        "4. **Use in training**: `model.fit(dataset)`\n",
        "\n",
        "So yes, you've understood it perfectly! `from_tensor_slices()` is the bridge between traditional array-based data and TensorFlow's powerful `tf.data` pipeline system.\n",
        "\n",
        "Made changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "## **Why use `from_tensor_slices()`?**\n",
        "\n",
        "✅ **Benefits of converting to `tf.data.Dataset`:**\n",
        "- **Batching**: `.batch(32)`\n",
        "- **Shuffling**: `.shuffle(1000)` \n",
        "- **Transformations**: `.map(preprocess_function)`\n",
        "- **Prefetching**: `.prefetch(tf.data.AUTOTUNE)`\n",
        "- **Performance optimizations**: Parallel processing, memory efficiency\n",
        "- **Consistent API**: Same interface as other TensorFlow datasets\n",
        "\n",
        "## **Summary of the workflow:**\n",
        "1. **Start with**: In-memory arrays (NumPy, lists, tensors)\n",
        "2. **Convert using**: `tf.data.Dataset.from_tensor_slices()`\n",
        "3. **Apply transformations**: `.batch()`, `.shuffle()`, `.map()`, etc.\n",
        "4. **Use in training**: `model.fit(dataset)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating tf.data.Dataset.from_tensor_slices() usage\n",
        "\n",
        "# Example 1: Simple NumPy arrays\n",
        "print(\"Example 1: Simple arrays\")\n",
        "X_simple = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "y_simple = np.array([0, 1, 0, 1])\n",
        "\n",
        "simple_ds = tf.data.Dataset.from_tensor_slices((X_simple, y_simple))\n",
        "print(f\"Created dataset from arrays: {simple_ds}\")\n",
        "\n",
        "# Now we can batch, shuffle, etc.\n",
        "batched_simple = simple_ds.batch(2)\n",
        "for batch_x, batch_y in batched_simple:\n",
        "    print(f\"Batch X: {batch_x.numpy()}\")\n",
        "    print(f\"Batch y: {batch_y.numpy()}\")\n",
        "    break\n",
        "\n",
        "print(\"\\nExample 2: CIFAR-10 arrays (your highlighted code)\")\n",
        "print(\"(x_train, y_train) are NumPy arrays → convert to tf.data.Dataset → apply transformations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory comparison: \n",
        "\n",
        "# Method 1: Loads full dataset into memory (your current approach)\n",
        "print(\"Method 1: tf.keras.datasets (loads all into memory)\")\n",
        "(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\n",
        "print(f\"x_train memory usage: {x_train.nbytes / (1024**2):.1f} MB\")\n",
        "print(f\"y_train memory usage: {y_train.nbytes / (1024**2):.1f} MB\")\n",
        "\n",
        "# Method 2: Streaming from disk (memory efficient)\n",
        "print(\"\\nMethod 2: tensorflow_datasets (streams from disk)\")\n",
        "import tensorflow_datasets as tfds\n",
        "train_stream = tfds.load('cifar10', split='train[:10%]', as_supervised=True, download=False)\n",
        "train_batched = train_stream.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"With tfds: Data is loaded batch by batch from disk during training\")\n",
        "print(\"Memory usage: Only one batch (64 images) in memory at a time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clarification: Different data loading methods\n",
        "\n",
        "**Important distinction between data loading approaches:**\n",
        "\n",
        "### 1. `tensorflow_datasets` (tfds)\n",
        "- **Pre-curated datasets**: CIFAR-10, ImageNet, MNIST, etc.\n",
        "- **Downloads from internet**: Not from your local folders\n",
        "- **Standard format**: All datasets follow same structure\n",
        "- **Example**: `tfds.load('cifar10')` gets CIFAR-10 from TensorFlow's servers\n",
        "\n",
        "### 2. `image_dataset_from_directory` (Keras utility)  \n",
        "- **Your custom images**: Reads from YOUR folder structure\n",
        "- **Expects class folders**: Each subfolder = one class\n",
        "- **Local files**: Works with images on your computer\n",
        "- **Example**: Reading cats_vs_dogs from your local folders\n",
        "\n",
        "### 3. `tf.keras.datasets`\n",
        "- **Built-in datasets**: Limited selection (CIFAR-10, MNIST, etc.)\n",
        "- **Loads into memory**: Downloads then loads all into RAM\n",
        "- **NumPy arrays**: Returns regular arrays, not tf.data.Dataset\n",
        "\n",
        "All can create `tf.data.Dataset` objects, but they get data from different sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example comparison of the three approaches:\n",
        "\n",
        "# 1. tensorflow_datasets - downloads CIFAR-10 from internet\n",
        "import tensorflow_datasets as tfds\n",
        "tfds_data = tfds.load('cifar10', split='train[:1%]', as_supervised=True, download=False)\n",
        "print(\"1. TFDS: Downloads standard datasets from TensorFlow servers\")\n",
        "\n",
        "# 2. Keras datasets - built-in datasets loaded into memory  \n",
        "import tensorflow as tf\n",
        "(x, y), _ = tf.keras.datasets.cifar10.load_data()\n",
        "keras_data = tf.data.Dataset.from_tensor_slices((x[:100], y[:100]))\n",
        "print(\"2. Keras datasets: Built-in datasets loaded into memory\")\n",
        "\n",
        "# 3. image_dataset_from_directory - YOUR folders (hypothetical example)\n",
        "# from tensorflow.keras.utils import image_dataset_from_directory\n",
        "# custom_data = image_dataset_from_directory(\n",
        "#     \"my_photos/\",  # YOUR folder with subfolders like cats/, dogs/\n",
        "#     image_size=(224, 224),\n",
        "#     batch_size=32\n",
        "# )\n",
        "print(\"3. image_dataset_from_directory: Reads YOUR custom folder structure\")\n",
        "\n",
        "print(\"\\nAll three can create tf.data.Dataset objects, but from different sources!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great question! This code does **both** - it loads the full dataset into memory first, then creates a batched processing pipeline:\n",
        "\n",
        "## What happens step by step:\n",
        "\n",
        "1. **`tf.keras.datasets.cifar10.load_data()`** \n",
        "   - ✅ **Loads ENTIRE dataset into memory** as NumPy arrays\n",
        "   - `x_train` shape: `(50000, 32, 32, 3)` - all 50,000 images in RAM\n",
        "   - `y_train` shape: `(50000, 1)` - all labels in RAM\n",
        "\n",
        "2. **`tf.data.Dataset.from_tensor_slices()`**\n",
        "   - Creates a dataset from the in-memory arrays\n",
        "   - The data is still in memory, but now wrapped in a `tf.data.Dataset`\n",
        "\n",
        "3. **`.batch(64).prefetch()`**\n",
        "   - ✅ **Processes in batches of 64** during training\n",
        "   - Only loads one batch at a time to the GPU/model\n",
        "   - `prefetch` prepares the next batch while current batch is processing\n",
        "\n",
        "## Memory vs Processing:\n",
        "\n",
        "- **Memory**: Full dataset loaded into RAM upfront\n",
        "- **Processing**: Batched processing during training\n",
        "\n",
        "## For truly memory-efficient loading:\n",
        "\n",
        "If you want to avoid loading everything into memory, use `tensorflow_datasets` or `image_dataset_from_directory`: \n",
        "\n",
        "\n",
        "## Summary:\n",
        "\n",
        "- **Your current code**: Loads ~600MB of CIFAR-10 into RAM, then processes in batches\n",
        "- **Alternative (tfds)**: Streams data from disk, only keeping one batch in memory\n",
        "\n",
        "For CIFAR-10 (small dataset), loading into memory is fine. For larger datasets (ImageNet, custom datasets), streaming approaches are essential to avoid running out of memory.\n",
        "\n",
        "The key difference is **where** the data lives before training, not **how** it's fed to the model during training.\n",
        "\n",
        "Made changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 1. **`image_dataset_from_directory` is a Keras utility**\n",
        "- It's part of `tensorflow.keras.utils`\n",
        "- As you can see in your code: `from tensorflow.keras.utils import image_dataset_from_directory`\n",
        "\n",
        "## 2. **It returns `tf.data.Dataset` objects**\n",
        "- The returned objects (`train_dataset`, `validation_dataset`, `test_dataset`) are indeed `tf.data.Dataset` objects\n",
        "- They have the same API and methods as datasets created with `tfds.load()` or `tf.data.Dataset.from_tensor_slices()`\n",
        "\n",
        "Let me add a cell to verify this: \n",
        "\n",
        "Edit [](file:///Users/marciero/Library/CloudStorage/Dropbox/DS-stats/DSC_DSP_courses/DSP_566/week_04_b/pretrained_models.ipynb)\n",
        "\n",
        "## Key Points:\n",
        "\n",
        "**Same ecosystem, different utilities:**\n",
        "- `tensorflow_datasets` (tfds): For accessing curated datasets\n",
        "- `keras.utils.image_dataset_from_directory`: For loading your own image folders\n",
        "- `tf.data.Dataset.from_tensor_slices()`: For creating datasets from arrays\n",
        "\n",
        "**All return `tf.data.Dataset` objects**, so you can:\n",
        "- Apply the same transformations (`.batch()`, `.map()`, `.shuffle()`, etc.)\n",
        "- Use them interchangeably in `model.fit()`\n",
        "- Chain operations in the same way\n",
        "\n",
        "**Why this matters:**\n",
        "- Consistent API across different data loading methods\n",
        "- Same performance optimizations (prefetching, parallel processing)\n",
        "- Same memory management patterns\n",
        "- Can mix and match approaches in the same project\n",
        "\n",
        "The Keras utility is essentially a convenience wrapper that creates `tf.data.Dataset` objects from folder structures, making it easy to work with your own image collections!\n",
        "\n",
        "Made changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".nbtools (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
